{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Uq_3t_tluNmU"
   },
   "source": [
    "# Multinomial Naive Bayes\n",
    "\n",
    "A Naive Bayes model assumes that each of the features it uses are conditionally independent of one another given some class. More formally, if I want to calculate the probability of observing features $ F_1 ... F_n $, given some class c, under the Naive Bayes assumption the following holds:\n",
    "\n",
    "$$ P(f_1,...,f_n|c)=\\prod_{i=1}^{n} P(f_i|c) $$\n",
    "\n",
    "This means that when I want to use a Naive Bayes model to classify a new example, the posterior probability is much simpler to work with:\n",
    "\n",
    "$$ P(c|f_1,...,f_n) ‚àù P(c)P(f_1|c)...P(f_n|c) $$\n",
    "\n",
    "Of course these assumptions of independence are rarely true, but in practice Naive Bayes models have performed surprisingly well, even on complex tasks where it is clear that the strong independence assumptions are false.\n",
    "\n",
    "Up to this point we have said nothing about the distribution of each feature. In other words, we have left $ p(f_i|c) $ undefined. The term Multinomial Naive Bayes simply lets us know that each $ p(f_i|c) $ is a multinomial distribution, rather than some other distribution (Gaussian, Bernoulli). This works well for data which can easily be turned into counts, such as word counts in text.\n",
    "\n",
    "So Naive Bayes classifier is a general term which refers to conditional independence of each of the features in the model, while Multinomial Naive Bayes classifier is a specific instance of a Naive Bayes classifier which uses a multinomial distribution for each of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit, RandomizedSearchCV, learning_curve, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "FEbVc4Ixxe6L"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.class_probs = None\n",
    "        self.log_label_word_proba = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # class counts and labels to get class prior (probability)\n",
    "        labels, labels_counts = np.unique(y, return_counts=True)\n",
    "        self.log_class_prior = np.log(labels_counts / y.size)\n",
    "        \n",
    "        # lable-word priors\n",
    "        vocab_size = X.shape[1]\n",
    "        label_vecs = {l: X[y == l] for l in labels} # vocab vector per label\n",
    "        label_word_count = {l: np.sum(label_vecs[l]) for l in labels} # word count per label\n",
    "        label_divisor = {l: label_word_count[l] + vocab_size + 1 for l in labels}\n",
    "        \n",
    "        # word_in_class_probabilities\n",
    "        label_word_proba = np.array([(np.sum(label_vecs[l], axis=0) + 1) / label_divisor[l] for l in labels])\n",
    "        label_word_proba = np.squeeze(label_word_proba)\n",
    "        self.log_label_word_proba = np.log(label_word_proba)\n",
    "        \n",
    "    def predict_log_proba(self, X):\n",
    "        return self.log_class_prior + np.array(X * self.log_label_word_proba.T)  \n",
    "\n",
    "    def predict(self, X):    \n",
    "        return np.argmax(self.predict_log_proba(X), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### We will now classify the \"20 newsgroups\" data set using our own naive bayes classifier and compare to the scikit learn built in version.\n",
    "\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). The split between the train and test set is based upon messages posted before and after a specific date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = fetch_20newsgroups(remove = ('headers', 'footer', 'quotes'), subset='train', return_X_y=True)\n",
    "X_test, y_test = fetch_20newsgroups(remove = ('headers', 'footer', 'quotes'), subset='test', return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def show_results(clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('Per Class Evaluation: \\n' + classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Compare our NaiveBayes to sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Class Evaluation: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.37      0.49       319\n",
      "           1       0.59      0.74      0.65       389\n",
      "           2       0.50      0.01      0.02       394\n",
      "           3       0.54      0.73      0.62       392\n",
      "           4       0.80      0.61      0.69       385\n",
      "           5       0.57      0.80      0.67       395\n",
      "           6       0.85      0.73      0.79       390\n",
      "           7       0.87      0.77      0.82       396\n",
      "           8       0.91      0.82      0.86       398\n",
      "           9       0.94      0.78      0.85       397\n",
      "          10       0.82      0.93      0.87       399\n",
      "          11       0.58      0.85      0.69       396\n",
      "          12       0.74      0.53      0.62       393\n",
      "          13       0.79      0.80      0.79       396\n",
      "          14       0.71      0.83      0.76       394\n",
      "          15       0.50      0.93      0.65       398\n",
      "          16       0.65      0.74      0.70       364\n",
      "          17       0.69      0.84      0.76       376\n",
      "          18       0.57      0.52      0.54       310\n",
      "          19       0.74      0.14      0.23       251\n",
      "\n",
      "    accuracy                           0.69      7532\n",
      "   macro avg       0.70      0.67      0.65      7532\n",
      "weighted avg       0.71      0.69      0.67      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handmade Naive bayes with CountVectorizer\n",
    "clf = make_pipeline(CountVectorizer(stop_words='english', ngram_range=(1,1)), NaiveBayes())\n",
    "show_results(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Class Evaluation: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.37      0.49       319\n",
      "           1       0.59      0.74      0.65       389\n",
      "           2       0.50      0.01      0.02       394\n",
      "           3       0.54      0.73      0.62       392\n",
      "           4       0.80      0.61      0.69       385\n",
      "           5       0.57      0.80      0.67       395\n",
      "           6       0.85      0.73      0.79       390\n",
      "           7       0.87      0.77      0.82       396\n",
      "           8       0.91      0.82      0.86       398\n",
      "           9       0.94      0.78      0.85       397\n",
      "          10       0.82      0.93      0.87       399\n",
      "          11       0.58      0.85      0.69       396\n",
      "          12       0.74      0.53      0.62       393\n",
      "          13       0.79      0.80      0.79       396\n",
      "          14       0.71      0.83      0.76       394\n",
      "          15       0.50      0.93      0.65       398\n",
      "          16       0.65      0.74      0.70       364\n",
      "          17       0.69      0.84      0.76       376\n",
      "          18       0.57      0.52      0.54       310\n",
      "          19       0.74      0.14      0.23       251\n",
      "\n",
      "    accuracy                           0.69      7532\n",
      "   macro avg       0.70      0.67      0.65      7532\n",
      "weighted avg       0.71      0.69      0.67      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sklearn Naive bayes with CountVectorizer\n",
    "clf = make_pipeline(CountVectorizer(stop_words='english', ngram_range=(1,1)), MultinomialNB())\n",
    "show_results(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "train_sizes, train_scores, test_scores = learning_curve(clf, X_train, y_train, cv=cv, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,1, figsize=(12, 8))\n",
    "axes.set_title('Learning Curves (Naive Bayes)')\n",
    "axes.set_ylim(*(0.01, 1.01))\n",
    "axes.set_xlabel('Examples')\n",
    "axes.set_ylabel('Score')\n",
    "  \n",
    "\n",
    "# Plot\n",
    "# axes.grid()\n",
    "axes.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "axes.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "axes.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "axes.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "axes.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Hyper-parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# RandomCV gives us similar results to GridSearch with a percentage of the run time\n",
    "\n",
    "param_grid = {\n",
    "    'tfidfvectorizer__ngram_range': [(1,1), (1,2), (1,3), (1,4)],\n",
    "    'tfidfvectorizer__binary': [True, False],\n",
    "    'tfidfvectorizer__norm': [None, 'l1', 'l2'],\n",
    "    'tfidfvectorizer__max_features': (5000, 10000, 20000,30000,40000,50000, None),\n",
    "}\n",
    "\n",
    "pipeline = make_pipeline(TfidfVectorizer(stop_words='english'), MultinomialNB())\n",
    "grid_search = RandomizedSearchCV(pipeline, param_grid, return_train_score=True, verbose=10, n_jobs=-1)\n",
    "\n",
    "clf_opt = grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "clf_opt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# We use best model to predict and print results  \n",
    "y_pred = clf_opt.predict(X_test)\n",
    "print('Per Class Evaluation: \\n' + classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NB exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
