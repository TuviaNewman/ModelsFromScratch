{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "tLXpoHg64HlD"
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "Random forest is a supervised learning algorithm. The \"forest\" it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of many \"dumb\" learning models can create one large \"smart\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TreeEnsemble():\n",
    "    '''\n",
    "    Tree ensemble is a class that holds many Decision Trees and uses their combined decision/vote to return a prediction.\n",
    "    X is a matrix of data values (rows are samples, columns are attributes)\n",
    "    y is a vector of corresponding target values\n",
    "    n_trees is the number of trees to create\n",
    "    sample_sz is the size of the sample set to use of each of the trees in the forest (chose the samples randomly, with or without repetition)\n",
    "    min_leaf is the minimal number of samples in each leaf node of each tree in the forest\n",
    "    '''\n",
    "    def __init__(self, n_trees, sample_sz, min_leaf):\n",
    "        self.trees = []\n",
    "        self.n_trees = n_trees\n",
    "        self.sample_sz = sample_sz\n",
    "        self.min_leaf = min_leaf\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        for tree_i in range(self.n_trees):\n",
    "            tree = DecisionTree(self.sample_sz, self.min_leaf)\n",
    "            self.trees.append(tree.fit(X, y))\n",
    "  \n",
    "    def predict(self, X):\n",
    "        pred = []\n",
    "        for tree in self.trees:\n",
    "            pred.append(tree.predict(X))\n",
    "        return np.asarray(pred).mean(axis=0)\n",
    "\n",
    "    def oob_mse(self):\n",
    "        '''\n",
    "        compute the mean squared error over all out of bag (oob) samples. That is, for each sample calculate the squared error using  predictions from \n",
    "        the trees that do not contain x in their respective bootstrap sample, then average this score for all samples.\n",
    "        '''\n",
    "        errors = []\n",
    "        for tree in self.trees:\n",
    "            errors.append(tree.oob_mse())\n",
    "        return np.asarray(errors).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "QA54r4DiQDkM"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    '''\n",
    "    A decision tree is a flowchart-like structure in which each internal node represents a question on an attribute (Taller than 1.5 meters?, Black Hair?),\n",
    "    each branch represents the outcome of the question on that datapoint, and each leaf node represents a class label \n",
    "    X is a matrix of data values (rows are samples, columns are attributes)\n",
    "    y is a vector of corresponding target values\n",
    "    sample_sz is the size of the sample set to use of each of the trees in the forest (chose the samples randomly, with or without repetition)\n",
    "    min_leaf is the minimal number of samples in each leaf node of each tree in the forest\n",
    "    '''\n",
    "    def __init__(self, sample_sz, min_leaf):\n",
    "        self.min_leaf = min_leaf\n",
    "        self.sample_size =  sample_sz\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # sample from X\n",
    "        num_samples = X.shape[0]\n",
    "        sample = np.random.randint(0, num_samples, self.sample_size)\n",
    "        self.X = X[sample]\n",
    "        self.y = y[sample]\n",
    "        not_sampled = [i for i in np.arange(num_samples) if i not in sample]\n",
    "        self.oob_X = X[not_sampled]\n",
    "        self.oob_y = y[not_sampled]\n",
    "        # call recursive builder\n",
    "        self.top_node = Node()\n",
    "        self.recursive_tree_builder(self.X, self.y, self.top_node)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # run on all trees and get proba per classes (make into vector and then avergae columns)\n",
    "        return np.apply_along_axis(self.predict_single, arr=X, axis=1)\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        node = self.top_node\n",
    "        val = node.value\n",
    "        feat_idx = node.feature\n",
    "        while(True):\n",
    "            if node.value is None or node.feature is None:\n",
    "                return node.mean\n",
    "            if x[feat_idx] > val:\n",
    "                node = node.bigger\n",
    "                val = node.value\n",
    "                feat_idx = node.feature\n",
    "            else:\n",
    "                node = node.smaller\n",
    "                val = node.value\n",
    "                feat_idx = node.feature\n",
    "                \n",
    "    def oob_mse(self):\n",
    "         return mean_squared_error(self.predict(self.oob_X), self.oob_y)\n",
    "    \n",
    "    def recursive_tree_builder(self, X, y, curr_node):\n",
    "        #if we have less than min leaf we return\n",
    "        if X.shape[0] <= 2 * self.min_leaf:\n",
    "            # update as end node with proba\n",
    "            curr_node.mean = np.mean(y)\n",
    "            return\n",
    "        else:\n",
    "            # find best feature to split by\n",
    "            curr_node.feature, curr_node.value = self.best_split(X, y)\n",
    "            if curr_node.value is None:\n",
    "                curr_node.mean = np.mean(y)\n",
    "                return\n",
    "            bigger = X[:,curr_node.feature] > curr_node.value\n",
    "            smaller = X[:,curr_node.feature] <= curr_node.value\n",
    "            self.recursive_tree_builder(X[bigger,:], y[bigger], curr_node.set_bigger())\n",
    "            self.recursive_tree_builder(X[smaller,:], y[smaller], curr_node.set_smaller())\n",
    "            return\n",
    "                \n",
    "    def best_split(self, X, y):\n",
    "        # for each feature we check 'all' points and take point with lowest\n",
    "        kwargs = {'y': y, 'min_leaf':self.min_leaf}\n",
    "        min_split_per_feature, error = np.apply_along_axis(self.get_min_split, arr=X, axis=0, **kwargs)\n",
    "        feature = np.argmin(error)\n",
    "        split_val = min_split_per_feature[feature]\n",
    "        return feature, split_val\n",
    "                                   \n",
    "    def get_min_split(self, feat, y, min_leaf):\n",
    "        idxs = np.argsort(feat)\n",
    "        feat = np.sort(feat)\n",
    "        y = y[idxs]\n",
    "        bounds = feat[min_leaf: -(min_leaf + 1)]\n",
    "\n",
    "        min_error = math.inf\n",
    "        split_val = None\n",
    "        for trial in bounds:\n",
    "            if self.bad_trial(trial, feat):\n",
    "                pass\n",
    "            error = self.get_var_error(trial, feat, y)\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                split_val = trial\n",
    "        return (split_val, min_error)\n",
    "\n",
    "    def bad_trial(self, split, feat):\n",
    "        bigger = feat > split\n",
    "        smaller = feat <= split\n",
    "        return (feat[bigger].shape[0] <= self.min_leaf) or (feat[smaller].shape[0] <= self.min_leaf)\n",
    "                                \n",
    "    def get_var_error(self, split, feat, y):\n",
    "        bigger = feat > split\n",
    "        smaller = feat <= split\n",
    "\n",
    "        var_bigger = np.square(np.var(feat[bigger]))\n",
    "        var_smaller = np.square(np.var(feat[smaller]))\n",
    "        \n",
    "        bigger_size = feat[bigger].shape[0]\n",
    "        smaller_size = feat[smaller].shape[0]\n",
    "        n = feat.shape[0]\n",
    "\n",
    "        return (bigger_size/n)*var_bigger + (smaller_size/n)*var_smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    This class represents a single node from a DecisionTree.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.value = None\n",
    "        self.smaller = None\n",
    "        self.bigger = None\n",
    "        self.mean = None\n",
    "\n",
    "    def set_bigger(self):\n",
    "        \"\"\"\n",
    "        Creates child, adds to child list and returns child\n",
    "        \"\"\"\n",
    "        self.bigger = Node()\n",
    "        return self.bigger\n",
    "                                   \n",
    "    def set_smaller(self):\n",
    "        \"\"\"\n",
    "        Creates child, adds to child list and returns child\n",
    "        \"\"\"\n",
    "        self.smaller = Node()\n",
    "        return self.smaller\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return self.smaller is None and self.bigger is None\n",
    "    \n",
    "    def print_tree(self, depth):\n",
    "        if self.is_leaf():\n",
    "             print(f'Probas: {self.mean}')\n",
    "        else:\n",
    "            print(f'Node level {depth}, Feature: {self.feature}, Split val: {self.value}')\n",
    "            self.smaller.print_tree(depth+1)\n",
    "            self.bigger.print_tree(depth+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Predict Boston Housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "te = TreeEnsemble(n_trees=2, sample_sz=100, min_leaf=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "te.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.3533754364991"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.oob_mse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "regressor = TreeEnsemble(n_trees=10, sample_sz=100, min_leaf=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "regressor.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.81583186544108"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.oob_mse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Manual Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_trees:1, sz:50, min_leaf:1 --- oob mse: 97.9634649122807\n",
      "n_trees:1, sz:50, min_leaf:5 --- oob mse: 71.90387728472858\n",
      "n_trees:1, sz:50, min_leaf:10 --- oob mse: 77.28887397660819\n",
      "\n",
      "n_trees:1, sz:100, min_leaf:1 --- oob mse: 65.8636701556629\n",
      "n_trees:1, sz:100, min_leaf:5 --- oob mse: 66.93963757416662\n",
      "n_trees:1, sz:100, min_leaf:10 --- oob mse: 68.47989952046046\n",
      "\n",
      "n_trees:1, sz:300, min_leaf:1 --- oob mse: 36.197046945469445\n",
      "n_trees:1, sz:300, min_leaf:5 --- oob mse: 56.80348075025134\n",
      "n_trees:1, sz:300, min_leaf:10 --- oob mse: 48.988806273653715\n",
      "\n",
      "n_trees:1, sz:500, min_leaf:1 --- oob mse: 45.48401552287581\n",
      "n_trees:1, sz:500, min_leaf:5 --- oob mse: 46.47288138847099\n",
      "n_trees:1, sz:500, min_leaf:10 --- oob mse: 44.26135525815943\n",
      "\n",
      "n_trees:5, sz:50, min_leaf:1 --- oob mse: 78.31511313582509\n",
      "n_trees:5, sz:50, min_leaf:5 --- oob mse: 71.12077151475054\n",
      "n_trees:5, sz:50, min_leaf:10 --- oob mse: 75.63303477311787\n",
      "\n",
      "n_trees:5, sz:100, min_leaf:1 --- oob mse: 71.62720517247911\n",
      "n_trees:5, sz:100, min_leaf:5 --- oob mse: 63.58641500802342\n",
      "n_trees:5, sz:100, min_leaf:10 --- oob mse: 64.1284508749088\n",
      "\n",
      "n_trees:5, sz:300, min_leaf:1 --- oob mse: 47.653719627253224\n",
      "n_trees:5, sz:300, min_leaf:5 --- oob mse: 52.784590243292214\n",
      "n_trees:5, sz:300, min_leaf:10 --- oob mse: 61.827868995804394\n",
      "\n",
      "n_trees:5, sz:500, min_leaf:1 --- oob mse: 51.165908295198776\n",
      "n_trees:5, sz:500, min_leaf:5 --- oob mse: 38.44302797176631\n",
      "n_trees:5, sz:500, min_leaf:10 --- oob mse: 57.44799398939008\n",
      "\n",
      "n_trees:10, sz:50, min_leaf:1 --- oob mse: 75.87281958893013\n",
      "n_trees:10, sz:50, min_leaf:5 --- oob mse: 74.0735178948196\n",
      "n_trees:10, sz:50, min_leaf:10 --- oob mse: 74.69387390231857\n",
      "\n",
      "n_trees:10, sz:100, min_leaf:1 --- oob mse: 67.04718045419636\n",
      "n_trees:10, sz:100, min_leaf:5 --- oob mse: 66.03078741758534\n",
      "n_trees:10, sz:100, min_leaf:10 --- oob mse: 64.07241546118165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in [1,5,10,20,50,100]:\n",
    "    for sz in [50,100,300,500]:\n",
    "        for min_leaf in [1,5, 10]:\n",
    "            forest = TreeEnsemble(n, sz, min_leaf)\n",
    "            forest.fit(X, y)\n",
    "            mse = forest.oob_mse()\n",
    "            print(f\"n_trees:{n}, sz:{sz}, min_leaf:{min_leaf} --- oob mse: {mse}\")\n",
    "            \n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Decision Trees - Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
