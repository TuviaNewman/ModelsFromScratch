{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "tLXpoHg64HlD"
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "Random forest is a supervised learning algorithm. The \"forest\" it builds, is an ensemble of decision trees. Decision trees classify the data points by sorting them down the tree from the root to some leaf, with the leaf node providing the classification of the example. The feature that we decide to split on is the feature that gives us the most information. Each node in the tree acts as a test case for some attribute, and each edge descending from the node corresponds to the possible answers to the test case. This process is recursive in nature and is repeated for every subtree rooted at the new node. \n",
    "\n",
    "In a random forest each decision tree in the forest considers a random subset of features as well as a random subset of the training data points. The makes each tree different in their own way and each tree will come to decisions using different features as well as different data points. When using a random forest to classify we run the data through every tree and then use the make a decision based on popular vote (classification) or average (regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes, fetch_california_housing\n",
    "\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Random Forest from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TreeEnsemble():\n",
    "    '''\n",
    "    Tree ensemble is a class that holds many Decision Trees and uses their combined decision/vote to return a prediction.\n",
    "    '''\n",
    "    def __init__(self, n_trees, sample_sz, min_leaf):\n",
    "        '''\n",
    "        n_trees is the number of trees to create\n",
    "        sample_sz is the size of the sample set to use of each of the trees in the forest (chose the samples randomly, with or without repetition)\n",
    "        min_leaf is the minimal number of samples in each leaf node of each tree in the forest\n",
    "        '''\n",
    "        self.trees = []\n",
    "        self.n_trees = n_trees\n",
    "        self.sample_sz = sample_sz\n",
    "        self.min_leaf = min_leaf\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Fit/train model\n",
    "        X: a matrix of data values (rows are samples, columns are attributes)\n",
    "        y: a vector of corresponding target values\n",
    "        '''\n",
    "        for tree_i in range(self.n_trees):\n",
    "            tree = DecisionTree(self.sample_sz, self.min_leaf)\n",
    "            self.trees.append(tree.fit(X, y))\n",
    "  \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict class labels using fitted/trained model\n",
    "        X: a matrix of data values (rows are samples, columns are attributes)\n",
    "        '''\n",
    "        pred = []\n",
    "        for tree in self.trees:\n",
    "            pred.append(tree.predict(X))\n",
    "        return np.asarray(pred).mean(axis=0)\n",
    "\n",
    "    def oob_mse(self):\n",
    "        '''\n",
    "        Compute the mean squared error over all out of bag (oob) samples. That is, for each sample calculate the squared error using  predictions from \n",
    "        the trees that do not contain x in their respective bootstrap sample, then average this score for all samples.\n",
    "        '''\n",
    "        errors = []\n",
    "        for tree in self.trees:\n",
    "            errors.append(tree.oob_mse())\n",
    "        return np.asarray(errors).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "QA54r4DiQDkM"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    '''\n",
    "    A decision tree is a flowchart-like structure in which each internal node represents a question on an attribute (Taller than 1.5 meters?, Black Hair?),\n",
    "    each branch represents the outcome of the question on that datapoint, and each leaf node represents a class label \n",
    "    '''\n",
    "    def __init__(self, sample_sz, min_leaf):\n",
    "        '''\n",
    "        sample_sz: amount of data to use when fitting tree\n",
    "        min_leaf: minimum amount of samples allowed in a leaf\n",
    "        '''\n",
    "        self.min_leaf = min_leaf\n",
    "        self.sample_size =  sample_sz\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X: a matrix of data values (rows are samples, columns are attributes)\n",
    "        y: a vector of corresponding target values\n",
    "        '''\n",
    "        # sample from X\n",
    "        num_samples = X.shape[0]\n",
    "        sample = np.random.randint(0, num_samples, self.sample_size)\n",
    "        self.X = X[sample]\n",
    "        self.y = y[sample]\n",
    "        not_sampled = [i for i in np.arange(num_samples) if i not in sample]\n",
    "        self.oob_X = X[not_sampled]\n",
    "        self.oob_y = y[not_sampled]\n",
    "        # call recursive builder\n",
    "        self.top_node = Node()\n",
    "        self.recursive_tree_builder(self.X, self.y, self.top_node)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict using fitted/trained model\n",
    "        X: a matrix of data values (rows are samples, columns are attributes)\n",
    "        '''\n",
    "        return np.apply_along_axis(self.predict_single, arr=X, axis=1)\n",
    "    \n",
    "    def predict_single(self, x):\n",
    "        '''\n",
    "        get prediction of a single datapoint for this tree\n",
    "        X: single row of data\n",
    "        '''\n",
    "        node = self.top_node\n",
    "        val = node.value\n",
    "        feat_idx = node.feature\n",
    "        while(True):\n",
    "            if node.value is None or node.feature is None:\n",
    "                return node.mean\n",
    "            if x[feat_idx] > val:\n",
    "                node = node.bigger\n",
    "                val = node.value\n",
    "                feat_idx = node.feature\n",
    "            else:\n",
    "                node = node.smaller\n",
    "                val = node.value\n",
    "                feat_idx = node.feature\n",
    "                \n",
    "    def oob_mse(self):\n",
    "        '''\n",
    "        Compute the mean squared error over all out of bag (oob) samples for this tree.\n",
    "        '''\n",
    "         return mean_squared_error(self.predict(self.oob_X), self.oob_y)\n",
    "    \n",
    "    def recursive_tree_builder(self, X, y, curr_node):\n",
    "        '''\n",
    "        X: a matrix of data values (rows are samples, columns are attributes)\n",
    "        y: a vector of corresponding target values\n",
    "        curr_node: current node in tree that we are working on.\n",
    "        '''\n",
    "        #if we have less than min leaf we return\n",
    "        if X.shape[0] <= 2 * self.min_leaf:\n",
    "            # update as end node with proba\n",
    "            curr_node.mean = np.mean(y)\n",
    "            return\n",
    "        else:\n",
    "            # find best feature to split by\n",
    "            curr_node.feature, curr_node.value = self.best_split(X, y)\n",
    "            if curr_node.value is None:\n",
    "                curr_node.mean = np.mean(y)\n",
    "                return\n",
    "            bigger = X[:,curr_node.feature] > curr_node.value\n",
    "            smaller = X[:,curr_node.feature] <= curr_node.value\n",
    "            self.recursive_tree_builder(X[bigger,:], y[bigger], curr_node.set_bigger())\n",
    "            self.recursive_tree_builder(X[smaller,:], y[smaller], curr_node.set_smaller())\n",
    "            return\n",
    "                \n",
    "    def best_split(self, X, y):\n",
    "        '''\n",
    "        X: a matrix of data values (rows are samples, columns are attributes)\n",
    "        y: a vector of corresponding target values\n",
    "        '''\n",
    "        # for each feature we check 'all' points and take point with lowest\n",
    "        kwargs = {'y': y, 'min_leaf':self.min_leaf}\n",
    "        min_split_per_feature, error = np.apply_along_axis(self.get_min_split, arr=X, axis=0, **kwargs)\n",
    "        feature = np.argmin(error)\n",
    "        split_val = min_split_per_feature[feature]\n",
    "        return feature, split_val\n",
    "                                   \n",
    "    def get_min_split(self, feat, y, min_leaf):\n",
    "        '''\n",
    "        get best split of data\n",
    "        feat: array of all of the features\n",
    "        y: a vector of corresponding target values\n",
    "        min_leaf: minimum amount of samples allowed in a leaf\n",
    "        '''\n",
    "        idxs = np.argsort(feat)\n",
    "        feat = np.sort(feat)\n",
    "        y = y[idxs]\n",
    "        bounds = feat[min_leaf: -(min_leaf + 1)]\n",
    "\n",
    "        min_error = math.inf\n",
    "        split_val = None\n",
    "        for trial in bounds:\n",
    "            if self.bad_trial(trial, feat):\n",
    "                pass\n",
    "            error = self.get_var_error(trial, feat, y)\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                split_val = trial\n",
    "        return (split_val, min_error)\n",
    "\n",
    "    def bad_trial(self, split, feat):\n",
    "        '''\n",
    "        This returns true if we cant split our node anymore\n",
    "        split: where the feature should be split\n",
    "        feat: single column of data (all datapoints for a single feature)\n",
    "        '''\n",
    "        bigger = feat > split\n",
    "        smaller = feat <= split\n",
    "        return (feat[bigger].shape[0] <= self.min_leaf) or (feat[smaller].shape[0] <= self.min_leaf)\n",
    "                                \n",
    "    def get_var_error(self, split, feat, y):\n",
    "        '''\n",
    "        split: where the feature should be split\n",
    "        feat: single column of data (all datapoints for a single feature)\n",
    "        y: a vector of corresponding target values\n",
    "        '''\n",
    "        bigger = feat > split\n",
    "        smaller = feat <= split\n",
    "\n",
    "        var_bigger = np.square(np.var(feat[bigger]))\n",
    "        var_smaller = np.square(np.var(feat[smaller]))\n",
    "        \n",
    "        bigger_size = feat[bigger].shape[0]\n",
    "        smaller_size = feat[smaller].shape[0]\n",
    "        n = feat.shape[0]\n",
    "\n",
    "        return (bigger_size/n)*var_bigger + (smaller_size/n)*var_smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "    This class represents a single node from a DecisionTree.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.value = None\n",
    "        self.smaller = None\n",
    "        self.bigger = None\n",
    "        self.mean = None\n",
    "\n",
    "    def set_bigger(self):\n",
    "        \"\"\"\n",
    "        Creates child, adds to child list and returns child\n",
    "        \"\"\"\n",
    "        self.bigger = Node()\n",
    "        return self.bigger\n",
    "                                   \n",
    "    def set_smaller(self):\n",
    "        \"\"\"\n",
    "        Creates child, adds to child list and returns child\n",
    "        \"\"\"\n",
    "        self.smaller = Node()\n",
    "        return self.smaller\n",
    "\n",
    "    def is_leaf(self):\n",
    "        '''\n",
    "        Returns true if node is leaf\n",
    "        '''\n",
    "        return self.smaller is None and self.bigger is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Predict on California Housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "X, y = fetch_california_housing(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "te = TreeEnsemble(n_trees=20, sample_sz=1000, min_leaf=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "te.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3244075290629032"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.oob_mse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### This is a pretty good result but we will see that we can do even better than this with Gradient Boosting Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Decision Trees - Exercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
